# Simple AI Agent Architecture Design

## Executive Summary

This document provides a comprehensive architecture design for simplifying Bazaar-Vid's AI agent system. The key principles are:
- **Minimal context** - Only pass what's absolutely necessary
- **Simple brain** - Just picks tools, no complex orchestration
- **Self-contained tools** - Each tool is responsible for its own execution
- **No sub-tools** - Flat architecture, tools don't call other tools
- **Sub-30 second generation** - Optimize for speed through simplicity

## Core Architecture Principles

### 1. Minimal Context (Tool-Specific)

**Current Problem**: Building massive context packets with everything "just in case"

**Solution**: Each tool declares exactly what context it needs

```typescript
// Tool Context Declaration
interface ToolContextRequirements {
  ADD: {
    required: ['userPrompt', 'projectId', 'sceneNumber'],
    optional: ['imageUrls', 'previousSceneCode']
  },
  EDIT: {
    required: ['userPrompt', 'projectId', 'sceneId', 'currentCode'],
    optional: ['imageUrls']
  },
  // ... other tools
}

// Context Builder becomes lightweight
class MinimalContextBuilder {
  async buildContextFor(tool: string, input: UserInput): Promise<ToolContext> {
    const requirements = ToolContextRequirements[tool];
    const context: ToolContext = {};
    
    // Only fetch what this specific tool needs
    for (const field of requirements.required) {
      context[field] = await this.fetchField(field, input);
    }
    
    // Optional fields only if present in input
    for (const field of requirements.optional) {
      if (this.isFieldRequested(field, input)) {
        context[field] = await this.fetchField(field, input);
      }
    }
    
    return context;
  }
}
```

### 2. Simple Brain (Tool Picker Only)

**Current Problem**: Complex orchestration logic, multi-step workflows, heavy LLM usage

**Solution**: Fast pattern matching with LLM fallback

```typescript
// Simple Brain Implementation
class SimpleBrain {
  // Pattern-based routing for common requests (sub-second)
  private patterns = {
    ADD: [
      /^(create|add|generate|make)\s+(a\s+)?(new\s+)?scene/i,
      /^(show|display|animate)\s+.*(text|image|video)/i,
      /with.*image/i  // When images are attached
    ],
    EDIT: [
      /^(change|modify|edit|update)\s+.*(scene|text|color|animation)/i,
      /^make\s+it\s+.*(bigger|smaller|faster|slower)/i,
      /^(remove|delete)\s+/i
    ],
    EXPORT: [
      /^(export|download|render|save)\s+/i
    ]
  };
  
  async pickTool(input: UserInput): Promise<ToolDecision> {
    // Step 1: Try pattern matching (0.001s)
    const tool = this.patternMatch(input.prompt);
    if (tool) {
      return {
        tool,
        confidence: 'high',
        reasoning: 'Pattern match',
        timeMs: 1
      };
    }
    
    // Step 2: Use fast LLM for ambiguous cases (2-3s)
    return this.fastLLMDecision(input);
  }
  
  private patternMatch(prompt: string): string | null {
    for (const [tool, patterns] of Object.entries(this.patterns)) {
      if (patterns.some(pattern => pattern.test(prompt))) {
        return tool;
      }
    }
    return null;
  }
  
  private async fastLLMDecision(input: UserInput): Promise<ToolDecision> {
    // Use smallest, fastest model (GPT-4o-mini or Claude Haiku)
    const response = await llm.complete({
      model: 'gpt-4o-mini',
      prompt: `Pick tool: ADD, EDIT, or EXPORT for: "${input.prompt}"`,
      maxTokens: 10,
      temperature: 0
    });
    
    return {
      tool: response.trim().toUpperCase(),
      confidence: 'medium',
      reasoning: 'LLM decision',
      timeMs: 2000
    };
  }
}
```

### 3. Self-Contained Tools (No Sub-Tools)

**Current Problem**: Tools calling other tools, complex dependencies

**Solution**: Each tool is completely independent

```typescript
// Base Tool Class - Self Contained
abstract class SelfContainedTool {
  abstract name: string;
  abstract contextRequirements: string[];
  
  // Each tool handles its own:
  // 1. Context fetching
  // 2. LLM calls
  // 3. Code generation
  // 4. Response formatting
  
  async execute(input: ToolInput): Promise<ToolOutput> {
    try {
      // 1. Validate input
      this.validateInput(input);
      
      // 2. Fetch only needed context
      const context = await this.fetchRequiredContext(input);
      
      // 3. Execute tool logic (no sub-tools!)
      const result = await this.performTask(input, context);
      
      // 4. Return standardized output
      return this.formatOutput(result);
      
    } catch (error) {
      return this.handleError(error);
    }
  }
  
  // Tools implement their own logic
  protected abstract performTask(
    input: ToolInput, 
    context: any
  ): Promise<any>;
}

// Example: ADD Tool Implementation
class AddTool extends SelfContainedTool {
  name = 'ADD';
  contextRequirements = ['projectId', 'sceneCount'];
  
  protected async performTask(input: ToolInput, context: any) {
    // Direct code generation - no layout step for speed
    if (context.sceneCount > 0 && context.lastSceneCode) {
      // Fast path: Use previous scene as template (10-15s)
      return this.generateWithTemplate(input, context.lastSceneCode);
    } else {
      // First scene: Generate from scratch (20-30s)
      return this.generateFromScratch(input);
    }
  }
  
  private async generateWithTemplate(input: ToolInput, template: string) {
    // Single LLM call with example
    const code = await llm.complete({
      model: 'claude-3-haiku', // Fast model
      prompt: `Generate scene similar to this template but for: "${input.prompt}"
      
Template:
${template}

New scene (keep same style):`,
      maxTokens: 2000,
      temperature: 0.7
    });
    
    return { code, timeMs: 12000 };
  }
}
```

### 4. Flat Architecture (No Sub-Tools)

**Current Problem**: Tools calling other tools creates complexity and latency

**Solution**: Each tool does everything it needs internally

```typescript
// BAD: Tool calling sub-tools
class OldAddTool {
  async execute(input) {
    const layout = await layoutTool.generate(input); // Sub-tool call
    const code = await codeTool.generate(layout);    // Another sub-tool
    const validated = await validatorTool.check(code); // Yet another
    return validated;
  }
}

// GOOD: Self-contained tool
class NewAddTool {
  async execute(input) {
    // All logic contained within this tool
    const code = await this.generateCode(input);
    const validated = this.validateCode(code); // Simple validation
    return { code, valid: validated };
  }
  
  private async generateCode(input) {
    // Direct generation - no intermediate steps
    return llm.complete({
      model: 'claude-3-haiku',
      prompt: this.buildPrompt(input),
      maxTokens: 2000
    });
  }
  
  private validateCode(code: string): boolean {
    // Basic syntax check - no external tool
    return code.includes('export default') && 
           code.includes('return (');
  }
}
```

## Optimized Request Flow

### Complete Flow: User Input â†’ Generated Scene (Target: <30s)

```typescript
// 1. Request Entry (0.1s)
async function handleGenerate(input: UserInput) {
  const startTime = Date.now();
  
  // 2. Simple Brain Decision (0.001s - 3s)
  const decision = await brain.pickTool(input);
  
  // 3. Minimal Context Fetch (1-2s)
  const context = await contextBuilder.buildContextFor(
    decision.tool, 
    input
  );
  
  // 4. Tool Execution (10-25s)
  const tool = toolRegistry.get(decision.tool);
  const result = await tool.execute({ ...input, context });
  
  // 5. Save Result (1-2s)
  const scene = await db.save(result);
  
  // 6. Background tasks (don't wait)
  fireAndForget(() => {
    uploadToStorage(scene);
    updateCache(scene);
    publishEvent(scene);
  });
  
  // Total: 12-32s (average ~22s)
  return {
    scene,
    timeMs: Date.now() - startTime
  };
}
```

## Speed Optimization Strategies

### 1. Model Selection by Speed

```typescript
const ModelSpeed = {
  // For brain/routing decisions
  ULTRA_FAST: 'gpt-4o-mini',        // 1-2s, $0.00015/1k
  
  // For simple edits/completions  
  FAST: 'claude-3-haiku-20240307',  // 3-5s, $0.00025/1k
  
  // For complex generation
  BALANCED: 'gpt-4o',               // 8-12s, $0.005/1k
  
  // Only for special cases
  POWERFUL: 'claude-3-5-sonnet',    // 15-20s, $0.003/1k
};

// Smart model selection
function selectModel(task: TaskType): string {
  switch(task.complexity) {
    case 'routing': return ModelSpeed.ULTRA_FAST;
    case 'simple_edit': return ModelSpeed.FAST;
    case 'generation': return ModelSpeed.BALANCED;
    case 'complex_scene': return ModelSpeed.POWERFUL;
  }
}
```

### 2. Caching Strategy

```typescript
class SpeedCache {
  // Cache common patterns
  private patternCache = new Map<string, ToolDecision>();
  private contextCache = new Map<string, any>();
  private templateCache = new Map<string, string>();
  
  // Pattern cache (permanent)
  cachePattern(prompt: string, decision: ToolDecision) {
    const key = this.normalizePrompt(prompt);
    this.patternCache.set(key, decision);
  }
  
  // Context cache (5 minutes)
  cacheContext(projectId: string, context: any) {
    this.contextCache.set(projectId, {
      data: context,
      expires: Date.now() + 300000 // 5 minutes
    });
  }
  
  // Template cache (1 hour)
  cacheTemplate(pattern: string, code: string) {
    this.templateCache.set(pattern, code);
  }
}
```

### 3. Parallel Processing

```typescript
// Parallelize independent operations
async function optimizedAddScene(input: AddToolInput) {
  // Start all independent operations
  const [
    projectContext,
    lastScene,
    userPreferences,
    templates
  ] = await Promise.all([
    getProjectContext(input.projectId),      // 0.5s
    getLastScene(input.projectId),           // 0.5s  
    getUserPreferences(input.userId),        // 0.3s
    getRelevantTemplates(input.prompt)       // 0.2s
  ]);
  
  // Total: 0.5s (parallel) vs 1.5s (sequential)
  
  // Now generate with all context available
  return generateScene({
    ...input,
    context: { projectContext, lastScene, userPreferences },
    templates
  });
}
```

## Implementation Architecture

### 1. Project Structure

```
src/
â”œâ”€â”€ brain/
â”‚   â””â”€â”€ simpleBrain.ts         # Tool picker only
â”œâ”€â”€ tools/
â”‚   â”œâ”€â”€ base.ts               # Base self-contained tool
â”‚   â”œâ”€â”€ add.ts                # ADD tool (no sub-tools)
â”‚   â”œâ”€â”€ edit.ts               # EDIT tool (no sub-tools)
â”‚   â””â”€â”€ export.ts             # EXPORT tool (no sub-tools)
â”œâ”€â”€ context/
â”‚   â””â”€â”€ minimalContext.ts     # Lightweight context builder
â”œâ”€â”€ cache/
â”‚   â””â”€â”€ speedCache.ts         # Performance caching
â””â”€â”€ router/
    â””â”€â”€ generation.ts         # Simple request router
```

### 2. Core Interfaces

```typescript
// Simple, focused interfaces
interface ToolInput {
  userPrompt: string;
  projectId: string;
  userId: string;
  // Tool-specific fields
  [key: string]: any;
}

interface ToolOutput {
  success: boolean;
  code?: string;
  error?: string;
  timeMs: number;
  cost: number;
}

interface BrainDecision {
  tool: 'ADD' | 'EDIT' | 'EXPORT';
  confidence: 'high' | 'medium' | 'low';
  reasoning: string;
  timeMs: number;
}

// No complex orchestration types!
```

### 3. Example Implementation

```typescript
// Complete minimal implementation
export class GenerationService {
  private brain = new SimpleBrain();
  private tools = new Map<string, BaseTool>();
  private cache = new SpeedCache();
  
  constructor() {
    // Register tools
    this.tools.set('ADD', new AddTool());
    this.tools.set('EDIT', new EditTool());
    this.tools.set('EXPORT', new ExportTool());
  }
  
  async generate(input: UserInput): Promise<GenerationResult> {
    const startTime = Date.now();
    
    try {
      // 1. Quick decision (cached or pattern match first)
      const decision = await this.brain.pickTool(input);
      
      // 2. Get the tool
      const tool = this.tools.get(decision.tool);
      if (!tool) throw new Error(`Unknown tool: ${decision.tool}`);
      
      // 3. Execute (tool handles everything internally)
      const result = await tool.execute(input);
      
      // 4. Return immediately (background saves)
      this.saveInBackground(result);
      
      return {
        ...result,
        totalTimeMs: Date.now() - startTime,
        decision
      };
      
    } catch (error) {
      return {
        success: false,
        error: error.message,
        totalTimeMs: Date.now() - startTime
      };
    }
  }
  
  private saveInBackground(result: ToolOutput) {
    // Don't await - fire and forget
    Promise.resolve().then(async () => {
      try {
        await db.saveScene(result);
        await storage.upload(result);
        await events.publish('scene.created', result);
      } catch (error) {
        logger.error('Background save failed', error);
      }
    });
  }
}
```

## Performance Benchmarks

### Current Architecture (Complex)
- Brain orchestration: 15-20s
- Tool selection: 5-10s  
- Context building: 5-10s
- Layout generation: 15-20s
- Code generation: 35-50s
- **Total: 75-110s**

### New Architecture (Simple)
- Pattern matching: 0.001s (or LLM: 2-3s)
- Minimal context: 1-2s
- Direct generation: 10-25s
- Background saves: 0s (don't wait)
- **Total: 11-30s (73% faster)**

### Cost Comparison
- Current: ~$0.065 per generation
- New: ~$0.008 per generation (88% cheaper)

## Migration Strategy

### Phase 1: Implement Simple Brain (Week 1)
1. Build pattern matcher
2. Add fast LLM fallback
3. Remove complex orchestration
4. Test with existing tools

### Phase 2: Refactor Tools (Week 2-3)
1. Remove sub-tool dependencies
2. Implement self-contained logic
3. Add direct generation paths
4. Optimize for speed

### Phase 3: Optimize & Monitor (Week 4)
1. Add caching layers
2. Implement parallel operations
3. Add performance monitoring
4. Fine-tune patterns

## Best Practices & Guidelines

### 1. Keep It Simple
- No complex orchestration
- No multi-step workflows
- No tool dependencies
- Direct path to result

### 2. Optimize for Speed
- Pattern match first
- Use fastest appropriate model
- Cache aggressively
- Parallelize when possible

### 3. Tool Independence
- Each tool is self-contained
- No shared state between tools
- Clear input/output contracts
- Handle own errors

### 4. Minimal Context
- Only fetch what's needed
- Lazy load optional data
- Cache frequently used context
- Avoid over-fetching

## Industry Best Practices Applied

### 1. **Microservice Principles**
- Single responsibility (one tool, one job)
- Loose coupling (no inter-tool dependencies)
- High cohesion (self-contained logic)

### 2. **Performance First**
- Measure everything
- Optimize the critical path
- Cache aggressively
- Fail fast

### 3. **Simple Architecture**
- Flat is better than nested
- Explicit is better than implicit
- Simple is better than complex

### 4. **Cost Optimization**
- Use appropriate models
- Cache LLM responses
- Batch when possible
- Monitor usage

## Conclusion

This simplified architecture achieves:
- **73% faster generation** (110s â†’ 30s)
- **88% lower costs** ($0.065 â†’ $0.008)
- **Simpler codebase** (no complex orchestration)
- **Better maintainability** (self-contained tools)
- **Industry best practices** (microservice patterns)

The key insight: **Most complexity in AI agents comes from over-engineering**. By keeping tools independent, context minimal, and decisions simple, we achieve better performance with less code.