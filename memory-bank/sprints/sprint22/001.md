
Let me review the codebase and design a practical implementation plan for your log analysis system. This will involve creating a separate log agent service that can analyze logs from various sources and provide useful feedback.

## Architecture Overview

### 1. Log-Agent Service (Port 3002)
A dedicated Node.js Express service that:
- Ingests logs from your main application
- Stores them in Redis with runId-based segmentation
- Performs analysis (regex patterns and OpenAI-powered deep analysis)
- Provides query endpoints and issue notifications

### 2. Dev-Agent Integration (Cursor Tools)
Integration with your editor through tools that:
- Query log data
- Control log lifecycle (clearing, refreshing)
- Receive notifications about detected issues

## Implementation Plan

### Step 1: Create the Log-Agent Service Structure

```typescript
// src/scripts/log-agent/server.ts
import express from 'express';
import { createClient } from 'redis';
import { Queue, Worker } from 'bullmq';
import { v4 as uuidv4 } from 'uuid';
import { OpenAI } from 'openai';

// Initialize Redis client
const redis = createClient({
  url: process.env.REDIS_URL || 'redis://localhost:6379'
});

// Initialize OpenAI client
const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY
});

// Initialize BullMQ queue
const logQueue = new Queue('log-analysis', {
  connection: redis
});

const app = express();
app.use(express.json());

// Log ingestion endpoint
app.post('/ingest', async (req, res) => {
  const { runId, source, lines } = req.body;
  
  if (!runId || !lines || !Array.isArray(lines)) {
    return res.status(400).json({ error: 'Invalid request format' });
  }
  
  // Store logs in Redis with TTL
  await redis.rPush(`logs:${runId}:${source}`, ...lines);
  await redis.expire(`logs:${runId}:${source}`, 60 * 60 * 24); // 24 hour TTL
  
  // Add to analysis queue
  await logQueue.add('analyze', {
    runId,
    source,
    batch: lines
  });
  
  res.status(200).json({ success: true });
});

// Control endpoint - clear logs and start fresh run
app.post('/control/clear', async (req, res) => {
  const { runId, callback } = req.body;
  
  if (runId) {
    // Get all keys for this runId
    const keys = await redis.keys(`logs:${runId}:*`);
    
    if (keys.length > 0) {
      await redis.del(keys);
    }
    
    // Clear issues for this runId
    await redis.del(`issues:${runId}`);
  }
  
  // Generate new runId
  const newRunId = new Date().toISOString().replace(/[-:.]/g, '');
  
  // Store callback URL if provided
  if (callback) {
    await redis.set(`callback:${newRunId}`, callback, { EX: 60 * 60 * 24 });
  }
  
  res.status(200).json({ runId: newRunId });
});

// Query endpoint
app.post('/qna', async (req, res) => {
  const { runId, question, grep } = req.body;
  
  if (!runId || !question) {
    return res.status(400).json({ error: 'Missing required parameters' });
  }
  
  // Get logs for this runId
  const keys = await redis.keys(`logs:${runId}:*`);
  let logContent = '';
  
  for (const key of keys) {
    const logs = await redis.lRange(key, 0, 1000);
    const source = key.split(':')[2];
    
    // Apply grep filter if provided
    let filteredLogs = logs;
    if (grep) {
      try {
        const regex = new RegExp(grep);
        filteredLogs = logs.filter(line => regex.test(line));
      } catch (e) {
        // Invalid regex, ignore filter
      }
    }
    
    if (filteredLogs.length > 0) {
      logContent += `\n=== ${source} Logs ===\n${filteredLogs.join('\n')}\n`;
    }
  }
  
  // Truncate to avoid excessive token usage
  if (logContent.length > 15000) {
    logContent = logContent.slice(0, 15000) + '\n[logs truncated...]';
  }
  
  try {
    // Use OpenAI to analyze logs based on the question
    const response = await openai.chat.completions.create({
      model: 'gpt-4o',
      messages: [
        {
          role: 'system',
          content: 'You are a log analysis expert. Analyze the logs provided and answer the question concisely. Focus on identifying errors, warning patterns, or issues that could cause the described problems.'
        },
        {
          role: 'user',
          content: `Question: ${question}\n\nLogs:\n${logContent}`
        }
      ],
      max_tokens: 1000
    });
    
    const answer = response.choices[0].message.content;
    res.status(200).send(answer);
  } catch (error) {
    console.error('Error calling OpenAI:', error);
    res.status(500).json({ error: 'Failed to analyze logs' });
  }
});

// Issues endpoint
app.get('/issues', async (req, res) => {
  const { runId } = req.query;
  
  if (!runId) {
    return res.status(400).json({ error: 'Missing runId parameter' });
  }
  
  // Get issues for this runId
  const issuesJson = await redis.get(`issues:${runId}`);
  const issues = issuesJson ? JSON.parse(issuesJson) : [];
  
  res.status(200).json(issues);
});

// Raw logs endpoint
app.get('/raw', async (req, res) => {
  const { runId, grep, limit = 2000 } = req.query;
  
  if (!runId) {
    return res.status(400).json({ error: 'Missing runId parameter' });
  }
  
  // Get logs for this runId
  const keys = await redis.keys(`logs:${runId}:*`);
  let results = [];
  
  for (const key of keys) {
    const source = key.split(':')[2];
    const logs = await redis.lRange(key, 0, Number(limit));
    
    // Apply grep filter if provided
    let filteredLogs = logs;
    if (grep) {
      try {
        const regex = new RegExp(String(grep));
        filteredLogs = logs.filter(line => regex.test(line));
      } catch (e) {
        // Invalid regex, ignore filter
      }
    }
    
    results.push(...filteredLogs.map(line => `[${source}] ${line}`));
  }
  
  // Limit total lines
  if (results.length > Number(limit)) {
    results = results.slice(0, Number(limit));
  }
  
  res.setHeader('Content-Type', 'text/plain');
  res.send(results.join('\n'));
});

// Health check endpoint
app.get('/health', (req, res) => {
  res.status(200).json({ status: 'ok' });
});

// Start server
const PORT = process.env.PORT || 3002;
app.listen(PORT, () => {
  console.log(`Log-Agent server running on port ${PORT}`);
});

// Connect to Redis
redis.connect().then(() => {
  console.log('Connected to Redis');
});

// Initialize worker
const worker = new Worker('log-analysis', async job => {
  const { runId, source, batch } = job.data;
  
  // Process batch for quick regex-based patterns
  const errorPatterns = [
    { regex: /Error: ECONNREFUSED/i, type: 'connection_refused', level: 'error' },
    { regex: /agent_creation_error/i, type: 'agent_creation_error', level: 'error' },
    { regex: /TaskProcessor graceful shutdown/i, type: 'task_processor_shutdown', level: 'warn' },
    { regex: /undefined is not a function/i, type: 'type_error', level: 'error' }
  ];
  
  const issues = [];
  
  for (const line of batch) {
    for (const pattern of errorPatterns) {
      if (pattern.regex.test(line)) {
        // Create fingerprint (simplified for example)
        const fingerprint = require('crypto')
          .createHash('sha1')
          .update(`${pattern.type}:${line.substring(0, 100)}`)
          .digest('hex')
          .substring(0, 10);
        
        // Check if already recorded
        const issuesJson = await redis.get(`issues:${runId}`);
        const existingIssues = issuesJson ? JSON.parse(issuesJson) : [];
        const existingIssue = existingIssues.find(i => i.fingerprint === fingerprint);
        
        if (existingIssue) {
          existingIssue.count += 1;
          existingIssue.lastSeen = new Date().toISOString();
        } else {
          existingIssues.push({
            level: pattern.level,
            type: pattern.type,
            fingerprint,
            summary: line.substring(0, 200),
            count: 1,
            source,
            firstSeen: new Date().toISOString(),
            lastSeen: new Date().toISOString(),
            notified: false
          });
        }
        
        await redis.set(`issues:${runId}`, JSON.stringify(existingIssues));
        
        // Get callback URL and notify if it exists and notification not sent
        const issueToNotify = existingIssues.find(i => 
          i.fingerprint === fingerprint && !i.notified && i.count >= 3
        );
        
        if (issueToNotify) {
          const callbackUrl = await redis.get(`callback:${runId}`);
          if (callbackUrl) {
            try {
              // Attempt notification
              const notification = {
                level: issueToNotify.level,
                type: issueToNotify.type,
                summary: issueToNotify.summary,
                count: issueToNotify.count,
                runId
              };
              
              // Update notification status
              issueToNotify.notified = true;
              await redis.set(`issues:${runId}`, JSON.stringify(existingIssues));
              
              // Send notification
              const fetch = require('node-fetch');
              fetch(callbackUrl, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify(notification),
                timeout: 1000 // Don't wait too long
              }).catch(err => console.error('Notification failed:', err));
              
            } catch (error) {
              console.error('Error sending notification:', error);
            }
          }
        }
        
        break; // Only register first matching pattern
      }
    }
  }
  
  return { processed: batch.length, issues: issues.length };
}, { connection: redis });

worker.on('completed', job => {
  console.log(`Job ${job.id} completed, processed ${job.returnvalue.processed} lines`);
});

worker.on('failed', (job, err) => {
  console.error(`Job ${job?.id} failed:`, err);
});
```

### Step 2: Add Log Shipping from Main Application

```typescript
// src/server/lib/log-agent-client.ts
import fetch from 'node-fetch';

export class LogAgentClient {
  private static instance: LogAgentClient;
  private runId: string;
  private endpoint: string;
  private buffers: Map<string, string[]>;
  private flushInterval: NodeJS.Timeout | null = null;
  
  private constructor() {
    this.runId = process.env.LOG_RUN_ID || new Date().toISOString().replace(/[-:.]/g, '');
    this.endpoint = process.env.LOG_AGENT_ENDPOINT || 'http://localhost:3002';
    this.buffers = new Map();
    
    // Set up flush interval
    this.flushInterval = setInterval(() => this.flushAll(), 5000);
  }
  
  public static getInstance(): LogAgentClient {
    if (!LogAgentClient.instance) {
      LogAgentClient.instance = new LogAgentClient();
    }
    return LogAgentClient.instance;
  }
  
  public log(source: string, message: string): void {
    const buffer = this.buffers.get(source) || [];
    buffer.push(`${new Date().toISOString()} ${message}`);
    this.buffers.set(source, buffer);
    
    // Flush if buffer gets large
    if (buffer.length >= 100) {
      this.flush(source);
    }
  }
  
  public async flush(source: string): Promise<void> {
    const buffer = this.buffers.get(source);
    if (!buffer || buffer.length === 0) return;
    
    try {
      await fetch(`${this.endpoint}/ingest`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          runId: this.runId,
          source,
          lines: buffer
        }),
        timeout: 2000
      });
      
      // Clear buffer on successful send
      this.buffers.set(source, []);
    } catch (error) {
      console.error(`Failed to send logs to agent for source ${source}:`, error);
    }
  }
  
  public async flushAll(): Promise<void> {
    const sources = Array.from(this.buffers.keys());
    for (const source of sources) {
      await this.flush(source);
    }
  }
  
  public setRunId(newRunId: string): void {
    this.runId = newRunId;
  }
  
  public getRunId(): string {
    return this.runId;
  }
  
  public dispose(): void {
    if (this.flushInterval) {
      clearInterval(this.flushInterval);
      this.flushInterval = null;
    }
  }
}

export const logAgent = LogAgentClient.getInstance();
```

### Step 3: Integrate with Existing Logging Systems

```typescript
// src/lib/logger.ts - Update to add log shipping

// Import the log agent client
import { logAgent } from '~/server/lib/log-agent-client';

// Add log shipping to Winston transport configurations
const createWinstonLogger = (options) => {
  const logger = winston.createLogger({
    // existing configuration...
    transports: [
      // existing transports...,
      
      // Add custom transport that forwards to log agent
      new winston.transports.Stream({
        stream: new stream.Writable({
          write(chunk, encoding, callback) {
            try {
              const logData = JSON.parse(chunk.toString());
              // Extract source/level and message from your log format
              logAgent.log(logData.source || 'app', `${logData.level}: ${logData.message}`);
            } catch (e) {
              // Fallback if parsing fails
              logAgent.log('app', chunk.toString());
            }
            callback();
          }
        })
      })
    ]
  });
  
  return logger;
};
```

### Step 4: Add Notification Endpoint to Main App

```typescript
// src/app/api/log-agent/notify/route.ts
import { NextResponse } from 'next/server';

export async function POST(request: Request) {
  try {
    const notification = await request.json();
    
    // Log the notification, you might want to store it or emit an event
    console.log('Log Agent Notification:', notification);
    
    // Here you could:
    // 1. Store in a notifications table
    // 2. Emit through your existing event system
    // 3. Update UI through WebSockets/Pusher
    
    // Respond immediately to not block the log agent
    return NextResponse.json({ received: true }, { status: 200 });
  } catch (error) {
    console.error('Error processing log agent notification:', error);
    return NextResponse.json(
      { error: 'Failed to process notification' },
      { status: 500 }
    );
  }
}
```

### Step 5: Create Script Commands

```typescript
// src/scripts/log-refresh.ts
import { execSync } from 'child_process';
import fetch from 'node-fetch';
import fs from 'fs';
import path from 'path';

async function refreshLogs() {
  // Get current runId from .env.local if it exists
  let currentRunId = '';
  const envPath = path.join(process.cwd(), '.env.local');
  
  if (fs.existsSync(envPath)) {
    const envContent = fs.readFileSync(envPath, 'utf8');
    const match = envContent.match(/LOG_RUN_ID=([^\s]+)/);
    if (match) {
      currentRunId = match[1];
    }
  }
  
  // Clear logs on the agent
  console.log('Clearing logs from previous run...');
  const response = await fetch('http://localhost:3002/control/clear', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
      runId: currentRunId,
      callback: 'http://localhost:3000/api/log-agent/notify'
    })
  });
  
  const { runId: newRunId } = await response.json();
  console.log(`New run ID: ${newRunId}`);
  
  // Update .env.local with new runId
  let envContent = fs.existsSync(envPath) 
    ? fs.readFileSync(envPath, 'utf8') 
    : '';
  
  if (envContent.includes('LOG_RUN_ID=')) {
    envContent = envContent.replace(/LOG_RUN_ID=([^\s]+)/, `LOG_RUN_ID=${newRunId}`);
  } else {
    envContent += `\nLOG_RUN_ID=${newRunId}\n`;
  }
  
  fs.writeFileSync(envPath, envContent);
  console.log('Updated .env.local with new run ID');
  
  // Optionally restart the dev server
  const hardReset = process.argv.includes('--hard');
  if (hardReset) {
    console.log('Performing hard reset...');
    try {
      execSync('npm run dev', { stdio: 'inherit' });
    } catch (error) {
      console.error('Error restarting dev server:', error);
    }
  }
  
  console.log('Log refresh complete!');
}

refreshLogs().catch(console.error);
```

```typescript
// src/scripts/log-query.ts
import fetch from 'node-fetch';
import fs from 'fs';
import path from 'path';

async function queryLogs() {
  // Get the question from command arguments
  const question = process.argv.slice(2).join(' ');
  
  if (!question) {
    console.error('Please provide a question. Usage: npm run log:ask "your question"');
    process.exit(1);
  }
  
  // Get current runId from .env.local if it exists
  let runId = '';
  const envPath = path.join(process.cwd(), '.env.local');
  
  if (fs.existsSync(envPath)) {
    const envContent = fs.readFileSync(envPath, 'utf8');
    const match = envContent.match(/LOG_RUN_ID=([^\s]+)/);
    if (match) {
      runId = match[1];
    }
  }
  
  if (!runId) {
    console.error('No LOG_RUN_ID found in .env.local. Please run npm run log:refresh first.');
    process.exit(1);
  }
  
  console.log(`Asking Log-Agent: "${question}"`);
  
  try {
    const response = await fetch('http://localhost:3002/qna', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        runId,
        question
      })
    });
    
    if (!response.ok) {
      console.error(`Error: ${response.status} ${response.statusText}`);
      process.exit(1);
    }
    
    const answer = await response.text();
    console.log('\nLog-Agent Response:');
    console.log('===================');
    console.log(answer);
  } catch (error) {
    console.error('Failed to query logs:', error);
    process.exit(1);
  }
}

queryLogs().catch(console.error);
```

### Step 6: Add Scripts to package.json

```json
{
  "scripts": {
    // ... existing scripts
    "log:agent": "tsx src/scripts/log-agent/server.ts",
    "log:refresh": "tsx src/scripts/log-refresh.ts",
    "log:ask": "tsx src/scripts/log-query.ts",
    "log:raw": "curl -s \"http://localhost:3002/raw?runId=$(grep LOG_RUN_ID .env.local | cut -d= -f2)&grep=$1\"",
    "log:issues": "curl -s \"http://localhost:3002/issues?runId=$(grep LOG_RUN_ID .env.local | cut -d= -f2)\" | jq"
  }
}
```

### Step 7: Create Cursor Integration Tools

Create a Cursor extension file or update your existing configuration to add log query tools:

```javascript
// log-tools.js (for use with Cursor or similar editor tool)
export const logTools = [
  {
    name: 'log_query',
    description: 'Ask Log-Agent a question about the current run\'s logs',
    parameters: {
      type: 'object',
      properties: {
        question: { type: 'string' },
        grep: { 
          type: 'string', 
          description: 'Optional server-side regex filter for logs'
        }
      },
      required: ['question']
    },
    execute: async ({ question, grep }) => {
      // Read runId from .env.local
      const fs = require('fs');
      const path = require('path');
      const envPath = path.join(process.cwd(), '.env.local');
      let runId = '';
      
      if (fs.existsSync(envPath)) {
        const envContent = fs.readFileSync(envPath, 'utf8');
        const match = envContent.match(/LOG_RUN_ID=([^\s]+)/);
        if (match) {
          runId = match[1];
        }
      }
      
      if (!runId) {
        return 'No active log run found. Please run npm run log:refresh first.';
      }
      
      // Call the Log-Agent
      const fetch = require('node-fetch');
      try {
        const response = await fetch('http://localhost:3002/qna', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            runId,
            question,
            grep
          })
        });
        
        return await response.text();
      } catch (error) {
        return `Error querying logs: ${error.message}. Make sure the Log-Agent is running at localhost:3002.`;
      }
    }
  },
  {
    name: 'log_clear',
    description: 'Flush current logs and start a fresh run',
    parameters: { 
      type: 'object', 
      properties: {
        hard: { 
          type: 'boolean', 
          description: 'Whether to also restart the dev server' 
        }
      }
    },
    execute: async ({ hard = false }) => {
      const { execSync, exec } = require('child_process');
      try {
        const command = hard ? 'npm run log:refresh -- --hard' : 'npm run log:refresh';
        const output = execSync(command, { encoding: 'utf8' });
        
        // Extract the new runId from the output
        const match = output.match(/New run ID: ([^\s]+)/);
        const newRunId = match ? match[1] : 'unknown';
        
        return `Logs cleared, new runId = ${newRunId}`;
      } catch (error) {
        return `Error clearing logs: ${error.message}`;
      }
    }
  }
];
```

## Docker Compose Setup

```yaml
# docker-compose.log-agent.yml
version: '3'

services:
  log-agent:
    build:
      context: .
      dockerfile: Dockerfile.log-agent
    ports:
      - "3002:3002"
    environment:
      - REDIS_URL=redis://redis:6379
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - redis
    volumes:
      - ./src:/app/src
    restart: unless-stopped

  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    restart: unless-stopped

volumes:
  redis-data:
```

## Dockerfile for Log Agent

```dockerfile
# Dockerfile.log-agent
FROM node:18-alpine

WORKDIR /app

COPY package.json package-lock.json ./
RUN npm install

COPY tsconfig.json ./
COPY src/scripts/log-agent ./src/scripts/log-agent

EXPOSE 3002

CMD ["npx", "tsx", "src/scripts/log-agent/server.ts"]
```

## Implementation Checklist

1. **Foundation (Day 1-2)**
   - Create Log-Agent service structure
   - Implement basic Redis storage for logs
   - Set up BullMQ worker for processing

2. **Log Shipping (Day 3)**
   - Integrate with your existing Winston logger
   - Create log-agent-client for batching and shipping logs
   - Test basic log ingestion

3. **Analysis Pipeline (Day 4-5)**
   - Implement regex pattern matching for common errors
   - Add OpenAI-powered deep analysis
   - Build notification system for detected issues

4. **CLI Tools (Day 6)**
   - Add npm scripts for log:refresh, log:ask, etc.
   - Test and refine command-line tools
   - Document usage for the team

5. **Editor Integration (Day 7-8)**
   - Create Cursor extension with log tools
   - Test tool calls from the editor
   - Optimize response formatting for editor display

6. **Optimization (Day 9-10)**
   - Add fingerprinting and deduplication
   - Implement cost-saving measures (compression, truncation)
   - Fine-tune notification thresholds

This implementation aligns with your requirements for a separate log analysis service that can provide real-time feedback, answer questions about logs, and notify the main application of issues. It leverages Redis for efficient storage, BullMQ for reliable processing, and integrates with OpenAI for intelligent analysis.
